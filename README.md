# AnalogCNN

## Abstract

This report explores an analog-based approach to CNN acceleration, emphasizing energy efficiency in resource-constrained embedded environments. The architecture integrates mixed-signal elements for CNN convolution, multiplication, accumulation, and activation, leveraging the power and speed of analog circuits. This design targets applications requiring low power consumption, real-time processing, and high-speed inference, such as embedded AI in image processing.

---

## 1. Introduction

The growth of AI and deep learning has amplified the demand for high-performance hardware accelerators capable of efficiently handling large computations. Convolutional Neural Networks (CNNs), in particular, are widely used in tasks like image recognition but often require significant computational power, leading to challenges in latency and power consumption. Traditional digital accelerators handle these computations but can be inefficient for real-time applications. This report presents a low-power analog accelerator that performs core CNN operations, including multiplication, accumulation, activation (ReLU), and pooling, while avoiding digital conversion where possible.

### Problem Statement

In digital CNN accelerators, the multiply-and-accumulate (MAC) operations consume considerable power and introduce latency due to digital-to-analog (D/A) and analog-to-digital (A/D) conversions. Analog circuits, by contrast, can perform these operations directly with minimal energy consumption and without the need for digital representation, potentially providing real-time inference with lower power consumption.

---

## 2. Circuit Design and Mathematical Formulation

### 2.1 Current-Steering Digital-to-Analog Converter (DAC)

The DAC converts binary digital values to an analog current, essential for initiating the convolution operation. Here, an 8-bit current-steering DAC is implemented.

**Schematic:**
![Current Steering DAC](images/dac_schematic.png)

#### Mathematical Formulation

An n-bit current-steering DAC outputs current \( I_{out} \) proportional to the binary input value \( D \) (in decimal) and the reference current \( I_{ref} \):

\[
I_{out} = \frac{D}{2^n} \times I_{ref}
\]

where:
- \( D \): Decimal representation of the binary input.
- \( n \): Number of bits in the DAC.
- \( I_{ref} \): Reference current, determined by an external source.

### 2.2 Multiplier Circuit Using Current Mirrors

The multiplier circuit leverages current mirrors to multiply the DAC output current with another analog input. In this configuration, current mirrors are used to duplicate and scale currents, allowing precise multiplication operations.

**Schematic:**
![Multiplier Circuit](images/multiplier_schematic.png)

#### Mathematical Formulation

For two input currents \( I_{DAC} \) and \( I_{in} \), the multiplication operation can be modeled as:

\[
I_{out} = I_{DAC} \times k
\]

where \( k \) is a scaling constant that depends on the transistor sizing and current mirror ratio in the circuit. 

### 2.3 Analog Integrator for Accumulation

The integrator sums the products generated by the multiplier circuit, combining results from multiple operations without A/D conversion, which reduces power and latency.

**Schematic:**
![Analog Integrator](images/integrator_schematic.png)

#### Mathematical Formulation

The integrator output voltage \( V_{out} \) over time \( t \) with an input current \( I_{in} \) and a capacitance \( C \) is given by:

\[
V_{out}(t) = \frac{1}{C} \int_0^t I_{in} \, dt + V_{initial}
\]

### 2.4 Max Pooling Circuit

The max pooling circuit performs down-sampling by retaining the maximum value within a region. This circuit outputs the highest current value from a set, representing the maximum activation in analog form.

**Schematic:**
![Max Pooling Circuit](images/max_pooling_schematic.png)

#### Mathematical Formulation

Given a set of input currents \( \{I_1, I_2, \dots, I_n\} \), the max pooling circuit outputs:

\[
I_{max} = \max(I_1, I_2, \dots, I_n)
\]

### 2.5 ReLU Circuit for Activation Function

The Rectified Linear Unit (ReLU) is implemented in analog form to introduce non-linearity into the network. The ReLU circuit blocks negative currents and outputs zero, while allowing positive currents to pass.

**Schematic:**
![ReLU Circuit](images/relu_schematic.png)

#### Mathematical Formulation

For an input current \( I_{in} \), the output current \( I_{out} \) of the ReLU circuit is defined as:

\[
I_{out} = \max(0, I_{in})
\]

---

## 3. Simulation Results

Simulations were conducted using Xschem to verify each circuit’s performance. The following subsections present the schematics and waveform results for each component:

### 3.1 Multiply and Accumulate Circuit Simulation

**Schematic and Waveform:**
![Multiply and Accumulate Circuit](images/mac_schematic.png)
![MAC Waveform](images/mac_waveform.png)

### 3.2 DAC Simulation

**Schematic and Waveform:**
![DAC Schematic](images/dac_schematic.png)
![DAC Waveform](images/dac_waveform.png)

### 3.3 ReLU Circuit Simulation

**Schematic and Waveform:**
![ReLU Schematic](images/relu_schematic.png)
![ReLU Waveform](images/relu_waveform.png)

### 3.4 Max Pooling Simulation

**Schematic and Waveform:**
![Max Pooling Schematic](images/max_pooling_schematic.png)
![Max Pooling Waveform](images/max_pooling_waveform.png)

---

## 4. Conclusion and Future Work

This analog accelerator design achieves efficient MAC operations for CNNs in a resource-constrained setting. Through analog implementation, the design offers reduced power consumption and latency compared to digital counterparts. Future work will include:
- Optimizing component sizes for further power efficiency.
- Expanding the circuit to support multi-channel convolutions.
- Evaluating performance with larger datasets and in diverse embedded environments.

---

## 5. References

1. Razavi, B., “The Current-Steering DAC [A Circuit for All Seasons],” IEEE Solid-State Circuits Magazine, vol. 10, no. 1, pp. 11-15, Winter 2018.
2. Asghar, M. S., et al., “A Digitally Controlled Analog Kernel for Convolutional Neural Networks,” ISOCC 2021.
3. Zhu, J., et al., “Analog Implementation of Reconfigurable Convolutional Neural Network Kernels,” APCCAS 2019.

---
